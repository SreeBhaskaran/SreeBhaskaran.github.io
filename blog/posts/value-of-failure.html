<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>The Value of Failure | Sreekumar R. Bhaskaran</title>
  <meta name="description" content="The Value of Failure: What Wordle Teaches Us About Experimentation" />

  <!-- Inter font (matches your site) -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome (icons) -->
  <link rel="stylesheet"
        href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.0/css/all.min.css">

  <!-- Your main site CSS -->
  <link rel="stylesheet" href="../../assets/style.css" />

  <!-- Post-specific styling -->
  <style>
    .post { max-width: 860px; margin: 0 auto; padding: 26px 0 50px; }
    .post-header h1 { margin: 0 0 10px; font-size: 34px; line-height: 1.15; }
    .post-meta { margin: 0 0 18px; color: var(--muted); font-size: 14px; display:flex; flex-wrap:wrap; gap:10px; align-items:center; }
    .tag { display:inline-flex; padding: 4px 10px; border-radius: 999px; border: 1px solid var(--border); background: rgba(53,76,161,.06); color: var(--text); font-size: 12px; }
    .post-section { padding: 14px 0; }
    .post-section h2 { margin: 22px 0 10px; font-size: 22px; }
    .post-section p { margin: 0 0 14px; }

    /* --- Featured Sources block --- */
    .sources-card{
      border: 1px solid var(--border);
      background: rgba(53,76,161,.05);
      border-radius: 14px;
      padding: 14px 14px;
      margin: 18px 0;
    }
    .sources-card h3{
      margin: 0 0 8px;
      font-size: 16px;
    }
    .sources-card ul{
      margin: 0;
      padding-left: 18px;
    }
    .sources-card li{
      margin: 8px 0;
      line-height: 1.4;
    }

    /* --- Quote callouts (pull quote style) --- */
.quote-callout{
  border: 1px solid var(--border);
  border-left: 5px solid var(--smu-blue);
  background: rgba(15,23,42,.02);
  border-radius: 14px;
  padding: 14px 16px;

  /* key changes */
  max-width: 720px;       /* not full width */
  margin: 18px auto;      /* center it */
}

.quote-callout.red-accent{
  border-left-color: var(--smu-red);
  background: rgba(204,0,53,.04);
}

  .quote-text{
  font-size: 16px;
  line-height: 1.55;
  margin-bottom: 10px;

  /* key change */
  color: rgba(15, 23, 42, .80);   /* slightly lighter than body text */
}

.quote-source{
  font-size: 13px;
  color: rgba(71, 85, 105, .85);  /* lighter muted */
  text-align: right;
}
.quote-text{
  font-style: italic;
  position: relative;
  padding-left: 18px;
}

.quote-text::before{
  content: "“";
  position: absolute;
  left: 0;
  top: -2px;
  font-size: 26px;
  line-height: 1;
  color: rgba(53,76,161,.35);
}

    /* --- Magazine inline images --- */
    figure.mag-img{
      margin: 12px 0 16px;
      border: 1px solid var(--border);
      border-radius: 14px;
      overflow: hidden;
      background: #fff;
    }
    figure.mag-img img{
      display:block;
      width:100%;
      height:auto;
    }
    figure.mag-img figcaption{
      padding: 10px 12px;
      font-size: 13px;
      color: var(--muted);
      border-top: 1px solid var(--border);
      background: rgba(15,23,42,.02);
      line-height: 1.35;
    }

    /* floated versions (text wrap) */
    figure.mag-img.float-right,
    figure.mag-img.float-left{
      width: 42%;
      max-width: 360px;
    }
    figure.mag-img.float-right{
      float:right;
      margin: 8px 0 14px 18px;
    }
    figure.mag-img.float-left{
      float:left;
      margin: 8px 18px 14px 0;
    }

    /* clear floats between sections */
    .clearfix::after{
      content:"";
      display:block;
      clear:both;
    }

    /* mobile: stack images */
    @media (max-width: 760px){
      figure.mag-img.float-right,
      figure.mag-img.float-left{
        float:none;
        width: 100%;
        max-width: 100%;
        margin: 14px 0;
      }
    }

    /* --- Scroll fade-in animation --- */
    .reveal{
      opacity: 0;
      transform: translateY(14px);
      transition: opacity .8s ease, transform .8s ease;
      will-change: opacity, transform;
    }
    .reveal.visible{
      opacity: 1;
      transform: translateY(0);
    }

    /* Respect reduced motion preferences */
    @media (prefers-reduced-motion: reduce){
      .reveal{
        opacity:1 !important;
        transform:none !important;
        transition:none !important;
      }
    }

    .refs { margin-top: 28px; padding-top: 18px; border-top: 1px solid var(--border); }
    .refs ul { margin: 8px 0 0; padding-left: 18px; }
    .refs li { margin: 10px 0; line-height: 1.45; }
  </style>
</head>

<body>

  <!-- Top-right nav only -->
  <header class="topbar">
  <div class="wrap">
    <nav class="nav nav-blog">
      <div class="nav-right">
        <a href="../../index.html" aria-label="Home">
          <span class="nav-ico" aria-hidden="true">
            <svg viewBox="0 0 24 24" fill="none">
              <path d="M3 10.5L12 3l9 7.5" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/>
              <path d="M6.5 10.5V21h11V10.5" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/>
            </svg>
          </span>
          <span class="nav-text">Home</span>
        </a>

        <a href="../../blog.html">
  <span class="nav-ico" aria-hidden="true">
    <svg viewBox="0 0 24 24" fill="none">
      <path d="M7 3h10v18H7z" stroke="currentColor" stroke-width="2" stroke-linejoin="round"/>
      <path d="M9 7h6" stroke="currentColor" stroke-width="2" stroke-linecap="round"/>
      <path d="M9 11h6" stroke="currentColor" stroke-width="2" stroke-linecap="round"/>
      <path d="M9 15h4" stroke="currentColor" stroke-width="2" stroke-linecap="round"/>
    </svg>
  </span>
  <span class="nav-text">Blog</span>
</a>

      </div>
    </nav>
  </div>
</header>


  <main class="wrap">
    <article class="post">

      <header class="post-header">
        <h1>The Value of Failure: What Wordle Teaches Us About Experimentation</h1>
        <p class="post-meta">
          <span class="tag">Experimentation</span>
          <span class="tag">AI</span>
          <span class="tag">Innovation</span>
          <span class="tag">Decision Making</span>
          <span>·</span>
          <time datetime="2025-12-25">December 25, 2025</time>
        </p>
      </header>

      <section class="post-section clearfix">

        <!-- A1 -->
        <figure class="mag-img float-right reveal">
          <img src="../assets/img/A1.jpg" alt="A magnifying glass next to a laptop">
          <figcaption>Experimentation is easy. Learning is harder.</figcaption>
        </figure>

        <p>
          Across industries, firms are running more experiments than ever—yet many are less sure what they are learning.
          With consumers becoming more discerning and competitive pressure to find what works, firms increasingly rely on pilots,
          A/B tests, and proof-of-concepts before committing meaningful resources. This is especially true in digital products,
          innovation, and AI—domains where the rules are still emerging and early feedback can be difficult to interpret.
        </p>

        <p>
          The logic is straightforward: experiment early, learn cheaply, and avoid costly mistakes later. Yet, as a recent
          <a href="https://www.wsj.com/articles/companies-had-fun-experimenting-with-ai-now-they-have-to-show-the-returns-2a683592" target="_blank" rel="noopener noreferrer"><em>Wall Street Journal</em> article</a>
          observed, many companies that “had fun experimenting” are now under pressure to demonstrate real returns from those efforts.
        </p>

        <div class="quote-callout">
          <div class="quote-text">
            Business technology leaders are winding down two years of fast-paced artificial intelligence experiments… and putting their AI dollars toward proven projects focused on return on investment.
          </div>
          <div class="quote-source">— <em>Wall Street Journal</em></div>
        </div>

        <p>
          Large investments in AI pilots haven’t resolved the problem. A
          <a href="https://www.forbes.com/councils/forbestechcouncil/2025/10/03/the-ai-strategy-thats-actually-working-why-95-of-enterprise-pilots-fail-and-how-to-join-the-5/"
             target="_blank" rel="noopener noreferrer"><em>Forbes</em> article</a>
          notes that despite billions spent on enterprise AI, most organizations struggle to move beyond the pilot stage,
          and even those that do often find meaningful returns elusive.
        </p>

        <div class="quote-callout red-accent">
          <div class="quote-text">
            Despite more than $30 billion in enterprise investment, most organizations can’t get past the pilot phase, and even those that do often find meaningful returns elusive.
          </div>
          <div class="quote-source">— <em>Forbes</em></div>
        </div>

        <p>
          Another industry report reinforces this: unclear objectives, insufficient data readiness, a lack of in-house expertise,
          and misguided pressure to greenlight proofs of concept are sinking many AI initiatives before they reach production
          (<a href="https://www.cio.com/article/3850763/88-of-ai-pilots-fail-to-reach-production-but-thats-not-all-on-it.html"
              target="_blank" rel="noopener noreferrer"><em>CIO</em></a>).
        </p>

        <div class="quote-callout">
          <div class="quote-text">
            Unclear objectives, insufficient data readiness, and a lack of in-house expertise are sinking many AI proofs of concept.
          </div>
          <div class="quote-source">— <em>CIO</em></div>
        </div>

        <div class="sources-card">
          <h3>Featured sources</h3>
          <ul>
            <li><a href="https://www.wsj.com/articles/companies-had-fun-experimenting-with-ai-now-they-have-to-show-the-returns-2a683592" target="_blank" rel="noopener noreferrer">Wall Street Journal — AI experimentation shifting toward ROI</a></li>
            <li><a href="https://www.forbes.com/councils/forbestechcouncil/2025/10/03/the-ai-strategy-thats-actually-working-why-95-of-enterprise-pilots-fail-and-how-to-join-the-5/" target="_blank" rel="noopener noreferrer">Forbes — Why 95% of enterprise AI pilots fail</a></li>
            <li><a href="https://www.cio.com/article/3850763/88-of-ai-pilots-fail-to-reach-production-but-thats-not-all-on-it.html" target="_blank" rel="noopener noreferrer">CIO — 88% of AI pilots fail to reach production</a></li>
          </ul>
        </div>

      </section>

      <section class="post-section clearfix">
        <h2>Where the Problem Lies: Method and Intent</h2>

        <!-- A2 -->
        <figure class="mag-img float-left reveal">
          <img src="../assets/img/A2.jpg" alt="Wall covered in sticky notes">
          <figcaption>Lots of activity doesn’t always translate into learning.</figcaption>
        </figure>

        <p>
          So where does the problem lie? It lies in both the method and the intent behind experimentation.
          Executives increasingly report pilots that generate mixed or contradictory signals, experiments that fail to scale,
          and early results that are difficult to interpret with confidence. In many cases, the challenge is not a lack of data—it is the nature
          of the information experiments generate. Early feedback is often noisy, incomplete, and easy to misread.
        </p>

        <p>
          The deployment of AI to automate processes and accelerate testing does not automatically solve these issues. If anything, it can make them worse:
          broad, general-purpose AI models can introduce additional noise into the experimentation process, especially when the underlying measurement systems
          are already imperfect. AI pilots rarely fail because the model didn’t work; they fail because foundations are weak: fragmented data, unsecured identities,
          and systems that don’t scale. Scaling isn’t about running more pilots. It’s about moving a smaller number of well-governed models into production with confidence.
        </p>
      </section>

      <section class="post-section clearfix">
        <h2>A Simple Lens: What Wordle Teaches About Learning</h2>

        <p>
          So how should firms design their experiments, and what strategy should guide them? One useful lens comes from the popular game <em>Wordle</em>.
          Finding the word through a series of guesses is, in many ways, like running experiments under uncertainty. The strategies that work—and those that fail—
          offer useful pointers for how firms should approach experimentation, especially when the goal is to learn rather than to “win” on the first try.
        </p>

        <p>
          Take, for example, a guess that appears to “work” early in the game—when you start with a word that places one or two letters correctly.
          That small success can feel reassuring, but it is often deceptively comforting because it does not meaningfully narrow the solution space.
          By contrast, a guess that fails completely can be far more informative because it provides a clean slate and forces you toward a different approach.
        </p>

        <blockquote class="quote-callout red-accent" style="margin-top:16px;">
          <div class="quote-text" style="margin-bottom:0;">
            Experimentation works only when actions are chosen with the intent to learn—not simply to be right on the first try.
          </div>
        </blockquote>
      </section>

      <section class="post-section clearfix">
        <h2>Why Firms Often Mislearn</h2>

        <!-- A3 -->
        <figure class="mag-img float-right reveal">
          <img src="../assets/img/A3.jpg" alt="Abstract wave pattern symbolizing noise and signal">
          <figcaption>Under uncertainty, outcomes blend signal and noise.</figcaption>
        </figure>

        <p>
          Many organizations treat experiments as a way to confirm what they already hope is true, rather than as a way to learn what they do not yet know.
          When early results look promising, confidence rises—even when the evidence is fragile. When results disappoint, initiatives are quickly abandoned—even when
          the failure itself is informative. The cost is not just wasted effort, but belief built on weak signals.
        </p>

        <p>
          Under uncertainty, that framing can be exactly backward: clear failures can teach more than ambiguous successes, especially early in the learning process.
          But early experiments rarely produce clean feedback—metrics are imperfect proxies, environments shift, and outcomes reflect a mix of signal and noise.
          Managers can end up drawing confident conclusions from evidence that does little to reduce uncertainty—one reason so many AI pilots generate activity without producing clarity.
        </p>
      </section>

      <section class="post-section clearfix">
        <h2>Noisy Experimentation: Why the Strategy Must Change</h2>

        <p>
          This is precisely what my co-author <a href="https://rady.ucsd.edu/faculty-research/faculty/sanjiv-erat.html" target="_blank" rel="noopener noreferrer">Sanjiv Erat</a> and I investigate in our research on noisy experimentation
          (<a href="https://pubsonline.informs.org/doi/abs/10.1287/msom.2024.1133" target="_blank" rel="noopener noreferrer">M&amp;SOM paper</a>).
          The core idea is that experimentation under significant noise requires a fundamentally different approach.
          While AI pilots allow firms to scale experimentation quickly, the signals they produce are often noisy—and, critically, the signal-to-noise ratio is not easy to establish in real time.
        </p>

        <p>
          When noise is high, running faster and more tests is not necessarily the solution. What matters is designing smarter tests that match the uncertainty
          and complexity of the use case. Smart organizations learn when to discount noise and when to act on a signal. In such settings, priors informed by human-in-the-loop
          judgment become even more important.
        </p>

        <p>
          There is a second reason experimentation can mislead: experiments don’t happen in a vacuum. As my co-author Sanjiv Erat and his colleagues
          (Ping-Chieh Huang and Zhe Zhang) have shown, experiments can change behavior by shaping incentives
          (<a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4976827" target="_blank" rel="noopener noreferrer">SSRN paper</a>).
          When people know they are being evaluated, they optimize for winning the evaluation, not for producing the best underlying outcome.
        </p>

        <!-- A4 -->
        <figure class="mag-img float-left reveal">
          <img src="../assets/img/A4.jpg" alt="Chess pawn representing strategy and incentives">
          <figcaption>Evaluation changes behavior: people optimize for the scorecard.</figcaption>
        </figure>

        <p>
          The authors’ model and empirical analysis of data science contests illustrate this dynamic clearly. Many firms run contest-style competitions,
          releasing training data while reserving a private test set to rank submissions. Statistically, an intermediate train–test split seems ideal:
          enough training data to build strong models, and enough test data to reliably rank them. But incentives change the picture. When the firm’s evaluation
          is most reliable—when it can most accurately distinguish high-performing from low-performing submissions—contestants have the strongest incentive to submit
          higher-variance “lottery ticket” models: approaches that are worse on average but occasionally score extremely well. This is exactly what an empirical analysis
          of Kaggle contests over the past five years uncovers. The implication is unsettling: firms may accurately pick a winner, but from a pool of inaccurate submissions.
        </p>
      </section>

      <section class="post-section clearfix">
        <h2>Experimentation in the Age of AI</h2>

        <!-- A5 -->
        <figure class="mag-img float-right reveal">
          <img src="../assets/img/A5.jpg" alt="Track lanes representing competition and scaling">
          <figcaption>The winners won’t test more—they’ll test smarter.</figcaption>
        </figure>

        <p>
          The business landscape has been changing at a rapid pace, and consumer needs are shifting even faster. As a result, competitive cycles are shorter and more demanding. It is in this backdrop that AI has rewritten the rules of product development—through wireframes, simulations, and rapid prototyping tools that allow teams to test more ideas than ever before.
        </p>

        <p>
          But the organizations that win won’t be those that simply test more. They will be those that test with intention—designing experiments that generate learning,
          recognizing when noise overwhelms signal, and building foundations strong enough to scale what works. Experimentation is no longer just a tool for discovery.
          In the age of AI, it is a strategic capability—and like Wordle, it rewards those who learn deliberately, not those who merely guess quickly.
        </p>
      </section>

      <footer class="refs clearfix">
        <h3>Sources &amp; References</h3>
        <ul>
          <li><a href="https://www.wsj.com/ARTICLE-LINK-HERE" target="_blank" rel="noopener noreferrer">Wall Street Journal — AI experimentation shifting toward ROI</a></li>
          <li><a href="https://www.forbes.com/councils/forbestechcouncil/2025/10/03/the-ai-strategy-thats-actually-working-why-95-of-enterprise-pilots-fail-and-how-to-join-the-5/" target="_blank" rel="noopener noreferrer">Forbes — Why 95% of enterprise AI pilots fail</a></li>
          <li><a href="https://www.cio.com/article/3850763/88-of-ai-pilots-fail-to-reach-production-but-thats-not-all-on-it.html" target="_blank" rel="noopener noreferrer">CIO — 88% of AI pilots fail to reach production</a></li>
          <li><a href="https://pubsonline.informs.org/doi/abs/10.1287/msom.2024.1133" target="_blank" rel="noopener noreferrer">Optimal Prototyping with Noisy Measurements (M&amp;SOM)</a></li>
          <li><a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4976827" target="_blank" rel="noopener noreferrer">Data Science Contests and the Perils of Intermediate Split Ratios (SSRN)</a></li>
        </ul>
      </footer>

    </article>
  </main>

  <!-- Scroll reveal script -->
  <script>
    (function(){
      const els = document.querySelectorAll('.reveal');
      if(!('IntersectionObserver' in window)){
        els.forEach(el => el.classList.add('visible'));
        return;
      }
      const io = new IntersectionObserver((entries) => {
        entries.forEach(entry => {
          if(entry.isIntersecting){
            entry.target.classList.add('visible');
            io.unobserve(entry.target);
          }
        });
      }, { threshold: 0.12 });
      els.forEach(el => io.observe(el));
    })();
  </script>

</body>
</html>
